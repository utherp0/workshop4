=== Introduction

This chapter provides a developer-centric view of the fundamentals and capabilities of the SDN (Software Defined Network) that is used within OpenShift for Application connectivity.

If you are not already logged on go to the UI URL at {{OPENSHIFT_CONSOLE_URL}}[{{OPENSHIFT_CONSOLE_URL}}, window="_blank"] and logon as '{{USER_ID}}' and password openshift. 

=== The basics of Service Addressing

*In this section we will create a couple of simple applications and show how they are represented via the Service and load-balancing of Pods within the SDN itself.*

If you do not already have a terminal tab running as defined in the pre-requisites please open one following the instructions in the pre-requisites

Ensure you are on the Administrator View (top level, select Administrator)

Select Home/Projects

Click on ‘Create Project’

For the project name give it networktest-{{USER_ID}}

Display Name and Description are optional

Click on 'Create'

Using the top left selection switch the UI from Administrator to Developer

In the Topology Tab click on ‘From Catalog’

In the search box enter ‘node’. Select the Node.js option (Builder Image)

Click ‘Create Application’

Default the builder image to version 12 (it should already have this selected)

For the git repo enter ‘https://github.com/utherp0/nodenews’ - insure the UI indicates 'Validated' when you leave the text field

Set the 'Application Name' to ‘application1-app’

Set the 'Name' to 'application1'

In Resources set the deployment type to DeploymentConfig

Click 'Create'

Click on ‘+Add’

Click on ‘From Catalog’

In the search box enter ‘node’. Again select the Node.js option (Builder Image)

Click ‘Create Application’ 

For the git repo enter ‘https://github.com/utherp0/nodenews’

Choose 'Create Application' from the Application pulldown

Set the 'Application Name' to 'application2-app'

Set the Name to ‘application2’

In Resources set the deployment type to DeploymentConfig

Click 'Create'

Wait until both applications have built and deployed. The Topology view will look similar to the image below:

image::sdn-1.png[Both Applications Loaded]

Once the applications have created change the mode from Developer to Administrator using the top left selection point

Click on 'Projects'. Select the networktest-{{USER_ID}} project

Click on 'Networking/Services'

image::sdn-2.png[Example Services]

TIP: The Services UI shows the services currently available in the Project. Note that there is a Service per application, so in this case there are two Services, application1 and application2. The Location is a singular IP address for each Service within the SDN - more on this later.

Click on 'Workloads/Deployment Configs'

Click on the application1 dc

Using the arrows next to the Pod indicator scale the application to three replicas

Click on ‘Pods’ in the DC view (not Workloads/Pods)

image::sdn-3.png[Pod Listing]

Click on the first active Pod for application1 listed

Scroll down and take a note of the IP address for the Pod

image::sdn-4.png[Highlighted Pod IP]

Click on 'Networking/Services'

Click on the application1 service

TIP: Note that the IP address for the Service has NOT changed. Scaling the Application has no impact on the singular IP address for the Service, which in actuality acts as a load-balancer across ALL of the IP addresses for the Pods of that Application.

=== Using the shorthand Service name directly

Click on 'Workloads/Pods'

Set the filter to 'Running' for ease of view

image::sdn-4b.png[filtered pods]

Click on the first running Pod for application1 (the name will be application1-1-(something)

Click on Terminal

In the Terminal window type:
[source]
----
curl http://localhost:8080
----

TIP: You will see the webpage for the Application as a return from the curl statement. The Container sees itself as localhost. Also note that because we are calling from within the Container we use the port address - if you were using the Routes their would be a Route for every Service, with no port address.

In the Terminal window type:

[source]
----
curl http://application1:8080
----

TIP: This is where it starts to get interesting. The Service 'name' itself is exported into the network namespace of the Container so it can refer to it as a short name. The SDN translates the service name into the service IP - in reality this Container could be getting a webpage back from any of the Application Pods that satisfy this Service.

=== Using the Fully Qualified Domain Name for accessing Services

In the Terminal window type:

[source]
----
curl http://application1.networktest-{{USER_ID}}.svc.cluster.local:8080
----

TIP: And this is the fully qualified version of the Service. by including the namespace/project name we can reach, effectively, any service on the SDN assuming the SDN has been configured to allow that. In this case we are just targeting our own Service from the application Container, now we will try the other application in the namespace.*

In the Terminal window hit the up arrow to get the last command, edit the name and change application1 to application2, hit return at the end of the statement

TIP: You should get the contents of a webpage. This is the output of the other application. This long format makes it easy to refer to other applications without having to leave and come back into the SDN (via a Route).

In the terminal type:

[source]
----
curl http://application2:8080
----

*We can also connect to any of the Services hosted within the namespace/project by default*

Ask the person sat next to you what their project name is and make a note of it

In the terminal type:

[source]
----
curl http://application1.(the project name from the person next to you).svc.cluster.local:8080
----

TIP: OpenShift Container Platform can be installed with two different modes of SDN. The first is subnet, which exposes all Services in all Namespace/Projects to each other. This instance has a subnet SDN which is why you should be able to call other peoples Services directly from your own via the internal FQDN address.

=== Controlling Access through Network Policies

.Network Policies
****
OpenShift actually provides three distinct levels of information when it comes to logging:Historically OpenShift had two ways of setting up the SDN it used, the first being 'subnet', which made the SDN flat and every namespace/project visible by default to every other one, and 'Multitenant' which isolated every namespace with its own network ID. This was deemed to be too coarse a control, so the concept of 'Network Policies' was created. This allows for rules to be applied to any object within a namespace in terms of ingress and exgress. 

By default when you create a project it is assigned some default policies that mirror the behaviour of the 'Multitenant' configuration, isolating the namespace. In this section we will remove those defaults and create some others to show the capabilities.
****

Click on 'Network/Network Policies'

For each of the policies listed click on the triple dot icon on the far right and choose ‘Delete Network Policy’.

image::sdn-5.png[Delete Network Policies]

The Network Policy tab should display ‘No Network Policies Found’.

Go to Workloads/Pods, click on one of the application1 Pods, choose Terminal

Repeat the ‘curl’ command listed above for the person sat next to you, i.e. curl their application1 

Ensure you get a webpage

Go to Networking/Network Policies

Click on ‘Create Network Policy’

Delete the contents of the YAML editor and replace it with the following:

[source]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
 name: example
 namespace: networktest-{{USER_ID}}
spec:
 podSelector:
   matchLabels:
     app: application1
 ingress: []
----

Click ‘Create’

Wait until the person next to you has done the same

Click on 'Workloads/Pods', click on one of the application1 Pods, choose Terminal

Repeat the ‘curl’ command listed above for the person sat next to you, i.e. curl their application1 

The call will eventually fail - feel free to hit Ctrl-C to interrupt

TIP: The creation of a Network Policy that prohibits ingress to the Application Service has stopped access to the Service from external namespaces AND internal Services.

Click on 'Workloads/Pods'

Click on the active pod for application2

Click on Terminal

Type:

[source]
----
curl http://application1:8080
----

The call will eventually fail

TIP: This shows that the Service is prohibited even from Services in its own namespace/project. This application of Network Policy allows for fine-grain control of traffic egress/ingress at the Service level. The other installation mode for SDN for OpenShift 4 is with Network Policies enabled, with default Network Policies providing a fully multitenanted environment.

Click on 'Projects'

On the triple dot icon on the far right for networktest-{{USER_ID}} select ‘Delete Project’

In the pop-up enter the name of the project (‘networktest-{{USER_ID}}’) and hit Delete

